import logging
import re
import aiohttp
import aiofiles
import hashlib
import json
import os
import requests
from pyrogram import Client, filters
from pyrogram.handlers import MessageHandler
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from datetime import datetime
import requests.utils as requests_utils

# Setup logging
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# Configuration
USER_DATA_FILE = 'user_data.json'
CHANNELS_FILE = 'authorized_channels.json'
SUDO_USERS_FILE = 'sudo_users.json'
OWNER_ID = 6556141430
MAX_FILE_SIZE = 45 * 1024 * 1024  # 45MB
CHECK_INTERVAL = 30  # Minutes

# Supported file types
DOCUMENT_EXTS = ['.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx', '.txt']
IMAGE_EXTS = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp']
ALLOWED_EXTS = DOCUMENT_EXTS + IMAGE_EXTS

def is_authorized_user(user_id):
    sudo_users = load_sudo_users()
    return user_id == OWNER_ID or user_id in sudo_users

def is_authorized_channel(channel_id):
    authorized_channels = load_channels()
    return channel_id in authorized_channels

def load_channels():
    try:
        with open(CHANNELS_FILE, 'r') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return []

def save_channels(channels):
    with open(CHANNELS_FILE, 'w') as f:
        json.dump(channels, f, indent=4)

def load_sudo_users():
    try:
        with open(SUDO_USERS_FILE, 'r') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return []

def save_sudo_users(sudo_users):
    with open(SUDO_USERS_FILE, 'w') as f:
        json.dump(sudo_users, f, indent=4)

def get_domain(url):
    parsed_uri = urlparse(url)
    return f"{parsed_uri.netloc}"

def load_user_data():
    try:
        with open(USER_DATA_FILE, 'r') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return {}

def save_user_data(user_data):
    with open(USER_DATA_FILE, 'w') as f:
        json.dump(user_data, f, indent=4)

def fetch_url_content(url):
    try:
        response = requests.get(url, timeout=15)
        response.raise_for_status()
        return response.text
    except Exception as e:
        logger.error(f"Error fetching {url}: {e}")
        return None

async def sanitize_filename(name):
    return re.sub(r'[\\/*?:"<>|]', '_', name).strip()

async def download_file(url, custom_name=None):
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url) as response:
                response.raise_for_status()
                
                # Determine filename
                if custom_name:
                    base_name = await sanitize_filename(custom_name)
                else:
                    base_name = await sanitize_filename(os.path.basename(urlparse(url).path))
                
                # Get extension
                content_type = response.headers.get('Content-Type', '')
                ext = os.path.splitext(urlparse(url).path)[1].lower() or \
                      ('.jpg' if 'image/jpeg' in content_type else
                       '.png' if 'image/png' in content_type else
                       '.gif' if 'image/gif' in content_type else
                       '.pdf' if 'application/pdf' in content_type else
                       '.docx' if 'vnd.openxmlformats' in content_type else '')
                
                filename = f"{base_name}{ext}"
                
                # Download and save
                async with aiofiles.open(filename, 'wb') as f:
                    await f.write(await response.read())
                    return filename
    except Exception as e:
        logger.error(f"Download error {url}: {e}")
        return None

def extract_files(html_content, base_url):
    soup = BeautifulSoup(html_content, 'lxml')
    files = []

    # Extract documents from links
    for link in soup.find_all('a', href=True):
        href = link['href']
        encoded_href = requests_utils.requote_uri(href)
        absolute_url = urljoin(base_url, encoded_href)
        link_text = link.text.strip()

        if any(absolute_url.lower().endswith(tuple(ALLOWED_EXTS)):
            if not link_text:
                filename = os.path.basename(absolute_url)
                link_text = os.path.splitext(filename)[0]
            file_type = 'document' if any(absolute_url.lower().endswith(tuple(DOCUMENT_EXTS)) else 'image'
            files.append({
                'name': link_text,
                'url': absolute_url,
                'type': file_type
            })

    # Extract images from img tags
    for img in soup.find_all('img', src=True):
        src = img['src']
        absolute_url = urljoin(base_url, src)
        alt_text = img.get('alt', '').strip()

        if any(absolute_url.lower().endswith(tuple(IMAGE_EXTS)):
            name = alt_text or os.path.splitext(os.path.basename(absolute_url))[0]
            files.append({
                'name': name,
                'url': absolute_url,
                'type': 'image'
            })

    return list({f['url']: f for f in files}.values())

async def create_document_file(url, files):
    domain = get_domain(url)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{domain}_files_{timestamp}.txt"

    with open(filename, 'w', encoding='utf-8') as f:
        for file in files:
            f.write(f"{file['type'].upper()}: {file['name']}\n{file['url']}\n\n")
    
    if os.path.getsize(filename) > MAX_FILE_SIZE:
        os.remove(filename)
        return None
    return filename

async def check_website_updates(client):
    user_data = load_user_data()
    for user_id, data in user_data.items():
        for url_info in data['tracked_urls']:
            url = url_info['url']
            stored_hash = url_info['hash']
            stored_files = url_info['files']

            current_content = fetch_url_content(url)
            if not current_content:
                continue

            current_hash = hashlib.sha256(current_content.encode()).hexdigest()
            current_files = extract_files(current_content, url)

            if current_hash != stored_hash:
                try:
                    await client.send_message(
                        chat_id=user_id,
                        text=f"üö® Website Updated: {url}"
                    )
                except Exception as e:
                    logger.error(f"Update send error: {e}")

                new_files = [f for f in current_files if f not in stored_files]

                if new_files:
                    # Send TXT summary
                    try:
                        txt_file = await create_document_file(url, new_files)
                        if txt_file:
                            await client.send_document(
                                chat_id=user_id,
                                document=txt_file,
                                caption=f"üìÑ New Files List ({len(new_files)})"
                            )
                            os.remove(txt_file)
                    except Exception as e:
                        logger.error(f"TXT send error: {e}")

                    # Send individual files
                    for file in new_files:
                        filename = None
                        try:
                            filename = await download_file(file['url'], file['name'])
                            if not filename:
                                raise Exception("Download failed")

                            await client.send_document(
                                chat_id=user_id,
                                document=filename,
                                caption=f"üÜï New {file['type'].capitalize()}:\n{file['name']}\n{file['url']}"
                            )
                            
                            # Update stored data
                            stored_files.append(file)
                            url_info['files'] = stored_files
                            url_info['hash'] = current_hash

                        except Exception as e:
                            await client.send_message(
                                chat_id=user_id,
                                text=f"‚ùå Error sending {file['name']}: {str(e)}"
                            )
                        finally:
                            if filename and os.path.exists(filename):
                                os.remove(filename)

                    save_user_data(user_data)

async def start(client, message):
    if message.chat.type == "private":
        if not is_authorized_user(message.from_user.id):
            await message.reply_text("‚ùå Unauthorized access.")
            return
    elif message.chat.type == "channel":
        if not is_authorized_channel(message.chat.id):
            await message.reply_text("‚ùå Unauthorized channel.")
            return
    else:
        await message.reply_text("‚ùå Command not allowed here.")
        return

    await message.reply_text(
        "üåê Website Tracker Bot\n\n"
        "Commands:\n"
        "/track <url> - Start tracking website\n"
        "/untrack <url> - Stop tracking\n"
        "/list - Show tracked websites\n"
        "/documents <url> - Get files list\n"
        "/help - Show help"
    )

async def track(client, message):
    # Authorization check
    if message.chat.type == "private":
        if not is_authorized_user(message.from_user.id):
            await message.reply_text("‚ùå Unauthorized.")
            return
    elif message.chat.type == "channel":
        if not is_authorized_channel(message.chat.id):
            await message.reply_text("‚ùå Unauthorized channel.")
            return
    else:
        await message.reply_text("‚ùå Command not allowed here.")
        return

    user_id = str(message.from_user.id or message.chat.id)
    url = ' '.join(message.command[1:]).strip()

    if not url.startswith(('http://', 'https://')):
        await message.reply_text("‚ö† Invalid URL format.")
        return

    user_data = load_user_data()
    if user_id not in user_data:
        user_data[user_id] = {'tracked_urls': []}

    if any(u['url'] == url for u in user_data[user_id]['tracked_urls']):
        await message.reply_text("‚ö† Already tracking this URL.")
        return

    content = fetch_url_content(url)
    if not content:
        await message.reply_text("‚ùå Failed to access URL.")
        return

    current_hash = hashlib.sha256(content.encode()).hexdigest()
    current_files = extract_files(content, url)

    user_data[user_id]['tracked_urls'].append({
        'url': url,
        'hash': current_hash,
        'files': current_files
    })
    save_user_data(user_data)

    # Send initial files
    if current_files:
        await message.reply_text(f"‚¨áÔ∏è Found {len(current_files)} files...")
        for file in current_files:
            filename = await download_file(file['url'], file['name'])
            if filename:
                try:
                    await client.send_document(
                        chat_id=user_id,
                        document=filename,
                        caption=f"üìÅ {file['type'].capitalize()}: {file['name']}\nüîó {file['url']}"
                    )
                finally:
                    if os.path.exists(filename):
                        os.remove(filename)
            else:
                await message.reply_text(f"‚ùå Failed to download {file['name']}")

    await message.reply_text(f"‚úÖ Tracking started: {url}\nFiles found: {len(current_files)}")

async def untrack(client, message):
    # Authorization check similar to track function
    
    user_id = str(message.from_user.id or message.chat.id)
    url = ' '.join(message.command[1:]).strip()

    user_data = load_user_data()
    if user_id not in user_data:
        await message.reply_text("‚ùå No tracked URLs.")
        return

    original_count = len(user_data[user_id]['tracked_urls'])
    user_data[user_id]['tracked_urls'] = [
        u for u in user_data[user_id]['tracked_urls']
        if u['url'] != url
    ]

    if len(user_data[user_id]['tracked_urls']) < original_count:
        save_user_data(user_data)
        await message.reply_text(f"‚úÖ Stopped tracking: {url}")
    else:
        await message.reply_text("‚ùå URL not found")

async def list_urls(client, message):
    # Authorization check
    
    user_id = str(message.from_user.id or message.chat.id)
    user_data = load_user_data()

    if user_id not in user_data or not user_data[user_id]['tracked_urls']:
        await message.reply_text("üì≠ No tracked URLs")
        return

    urls = "\n".join([u['url'] for u in user_data[user_id]['tracked_urls']])
    await message.reply_text(f"üìú Tracked URLs:\n\n{urls}")

async def list_documents(client, message):
    # Authorization check
    
    user_id = str(message.from_user.id or message.chat.id)
    url = ' '.join(message.command[1:]).strip()

    user_data = load_user_data()
    if user_id not in user_data or not user_data[user_id]['tracked_urls']:
        await message.reply_text("‚ùå No tracked URLs")
        return

    url_info = next((u for u in user_data[user_id]['tracked_urls'] if u['url'] == url), None)
    if not url_info:
        await message.reply_text("‚ùå URL not tracked")
        return

    files = url_info.get('files', [])
    if not files:
        await message.reply_text(f"‚ÑπÔ∏è No files found at {url}")
    else:
        try:
            txt_file = await create_document_file(url, files)
            await client.send_document(
                chat_id=user_id,
                document=txt_file,
                caption=f"üìë Files at {url} ({len(files)})"
            )
            os.remove(txt_file)
        except Exception as e:
            logger.error(f"Document list error: {e}")
            await message.reply_text("‚ùå Error sending file list")

# Add other management commands (addchannel, removesudo etc.) as needed

def main():
    app = Client(
        "my_bot",
        api_id=os.getenv("API_ID"),
        api_hash=os.getenv("API_HASH"),
        bot_token=os.getenv("BOT_TOKEN")
    )

    handlers = [
        MessageHandler(start, filters.command("start")),
        MessageHandler(track, filters.command("track")),
        MessageHandler(untrack, filters.command("untrack")),
        MessageHandler(list_urls, filters.command("list")),
        MessageHandler(list_documents, filters.command("documents")),
        # Add other handlers
    ]

    for handler in handlers:
        app.add_handler(handler)

    scheduler = AsyncIOScheduler()
    scheduler.add_job(check_website_updates, 'interval', minutes=CHECK_INTERVAL, args=[app])
    scheduler.start()

    try:
        app.run()
    except Exception as e:
        logger.error(f"Bot startup failed: {e}")

if __name__ == '__main__':
    main()
